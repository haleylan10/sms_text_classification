{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding; utf-8\n",
    "\"\"\"\n",
    "将从网络上下载的xml格式的wiki百科训练语料转为txt格式\n",
    "\"\"\"\n",
    "\n",
    "from gensim.corpora import WikiCorpus\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    print('主程序开始...')\n",
    "\n",
    "    input_file_name = 'zhwiki-latest-pages-articles.xml.bz2'\n",
    "    output_file_name = 'wiki.cn.txt'\n",
    "    print('开始读入wiki数据...')\n",
    "    input_file = WikiCorpus(input_file_name, lemmatize=False, dictionary={})\n",
    "    print('wiki数据读入完成！')\n",
    "    output_file = open(output_file_name, 'w', encoding=\"utf-8\")\n",
    "\n",
    "    print('处理程序开始...')\n",
    "    count = 0\n",
    "    for text in input_file.get_texts():\n",
    "        output_file.write(' '.join(text) + '\\n')\n",
    "        count = count + 1\n",
    "        if count % 10000 == 0:\n",
    "            print('目前已处理%d条数据' % count)\n",
    "    print('处理程序结束！')\n",
    "\n",
    "    output_file.close()\n",
    "    print('主程序结束！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import zhconv\n",
    "\n",
    "print('主程序执行开始...')\n",
    "\n",
    "input_file_name = 'wiki.cn.txt'\n",
    "output_file_name = 'wiki.cn.simple.txt'\n",
    "input_file = open(input_file_name, 'r', encoding='utf-8')\n",
    "output_file = open(output_file_name, 'w', encoding='utf-8')\n",
    "\n",
    "print('开始读入繁体文件...')\n",
    "lines = input_file.readlines()\n",
    "print('读入繁体文件结束！')\n",
    "\n",
    "print('转换程序执行开始...')\n",
    "count = 1\n",
    "for line in lines:\n",
    "    output_file.write(zhconv.convert(line, 'zh-hans'))\n",
    "    count += 1\n",
    "    if count % 10000 == 0:\n",
    "        print('目前已转换%d条数据' % count)\n",
    "print('转换程序执行结束！')\n",
    "\n",
    "print('主程序执行结束！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "import jieba\n",
    "\n",
    "print('主程序执行开始...')\n",
    "\n",
    "input_file_name = 'wiki.cn.simple.txt'\n",
    "output_file_name = 'wiki.cn.simple.separate.txt'\n",
    "input_file = open(input_file_name, 'r', encoding='utf-8')\n",
    "output_file = open(output_file_name, 'w', encoding='utf-8')\n",
    "\n",
    "print('开始读入数据文件...')\n",
    "lines = input_file.readlines()\n",
    "print('读入数据文件结束！')\n",
    "\n",
    "print('分词程序执行开始...')\n",
    "count = 1\n",
    "for line in lines:\n",
    "    # jieba分词的结果是一个list，需要拼接，但是jieba把空格回车都当成一个字符处理\n",
    "    output_file.write(' '.join(jieba.cut(line.split('\\n')[0].replace(' ', ''))) + '\\n')\n",
    "    count += 1\n",
    "    if count % 10000 == 0:\n",
    "        print('目前已分词%d条数据' % count)\n",
    "print('分词程序执行结束！')\n",
    "\n",
    "print('主程序执行结束！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "import re\n",
    "\n",
    "print('主程序执行开始...')\n",
    "\n",
    "input_file_name = 'wiki.cn.simple.separate.txt'\n",
    "output_file_name = 'wiki.txt'\n",
    "input_file = open(input_file_name, 'r', encoding='utf-8')\n",
    "output_file = open(output_file_name, 'w', encoding='utf-8')\n",
    "\n",
    "print('开始读入数据文件...')\n",
    "lines = input_file.readlines()\n",
    "print('读入数据文件结束！')\n",
    "\n",
    "print('分词程序执行开始...')\n",
    "count = 1\n",
    "cn_reg = '^[\\u4e00-\\u9fa5]+$'\n",
    "\n",
    "for line in lines:\n",
    "    line_list = line.split('\\n')[0].split(' ')\n",
    "    line_list_new = []\n",
    "    for word in line_list:\n",
    "        if re.search(cn_reg, word):\n",
    "            line_list_new.append(word)\n",
    "    print(line_list_new)\n",
    "    output_file.write(' '.join(line_list_new) + '\\n')\n",
    "    count += 1\n",
    "    if count % 10000 == 0:\n",
    "        print('目前已分词%d条数据' % count)\n",
    "print('分词程序执行结束！')\n",
    "\n",
    "print('主程序执行结束！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print('主程序开始执行...')\n",
    "\n",
    "    input_file_name = 'wiki.txt'\n",
    "    model_file_name = 'wiki.model'\n",
    "\n",
    "    print('转换过程开始...')\n",
    "    model = Word2Vec(LineSentence(input_file_name),\n",
    "                     size=400,  # 词向量长度为400\n",
    "                     window=5,\n",
    "                     min_count=5,\n",
    "                     workers=multiprocessing.cpu_count())\n",
    "    print('转换过程结束！')\n",
    "\n",
    "    print('开始保存模型...')\n",
    "    model.save(model_file_name)\n",
    "    print('模型保存结束！')\n",
    "\n",
    "    print('主程序执行结束！')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
